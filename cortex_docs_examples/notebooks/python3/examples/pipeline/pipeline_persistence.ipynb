{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Persistence\n",
    "\n",
    "### Persistence\n",
    "A pipeline persists its steps, context and dependencies. This example shows how those features of a pipeline work.\n",
    "\n",
    "To set up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from cortex import Cortex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create a Cortex Builder instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cortex = Cortex.local()\n",
    "builder = cortex.builder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the test pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tp = builder.pipeline('test_pipeline')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline's have the method `to_camel()` that produces a textual representation of the state of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp.to_camel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [basic](./pipeline_basic.ipynb) function of a pipeline is to use a series of steps to transform a [pandas DataFrame](https://pandas.pydata.org/pandas-docs/stable/dsintro.html#dataframe). To demonstrate persistence, we create a dataframe and add steps to our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# two columns of random numbers, indexed a through e\n",
    "s = pd.Series(np.random.randn(5), index=['a', 'b', 'c', 'd', 'e'])\n",
    "q = pd.Series(np.random.randn(5), index=['a', 'b', 'c', 'd', 'e'])\n",
    "\n",
    "# make a data frame by composing the columns together and labeling them\n",
    "df = pd.DataFrame({'c1':s,'c2':q})\n",
    "\n",
    "# display the result\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a step to the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_one(pipeline, df):\n",
    "    df['c1'] = df['c1'] + 1\n",
    "    \n",
    "tp.add_step(add_one)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The camel representation of the test pipeline shows the Base 64 encoded `add_one` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp.to_camel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is pipeline context?\n",
    "\n",
    "A pipeline's context is a dictionary of values or functions that could be used to operate on the dataframe when each pipeline step is run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp.set_context('one_median', df['c1'].astype(float).median())\n",
    "tp.to_camel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we add a step that uses the context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_score(pipeline, df):\n",
    "    mu = pipeline.get_context('one_median')\n",
    "    df['c3'] = df['c1'] - mu # no need to divide, standard deviation is 1\n",
    "    \n",
    "tp.add_step(standard_score)\n",
    "\n",
    "tp.run(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependecies\n",
    "\n",
    "Pipelines can be chained together, running one after another. The mechanism to do this is the pipeline dependencies function.\n",
    "\n",
    "Create another pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtp = builder.pipeline('depends_on_test_pipeline')\n",
    "\n",
    "def diff_c2_c3(pipeline, df):\n",
    "    df['c4'] = df['c2'] - df['c3']\n",
    "    \n",
    "\n",
    "dtp.add_step(diff_c2_c3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C3 must exist before this step is run, so `dtp` depends on `tp`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtp.add_dependency(tp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that a second pipeline is set up, run `dtp` to cause `tp` to run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtp.run(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Builders vs Datasets\n",
    "\n",
    "Datasets can have pipelines and datasets persist between kernel invocations, so pipelines associated with a dataset are persisted. Pipelines constructed from a builder are only persisted in memory. If the kernel restarts, the builders are deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = builder.dataset('sample').title('Persistence Sample Data')\\\n",
    "    .from_df(df).build()\n",
    "\n",
    "data_frame = data_set.as_pandas()\n",
    "data_frame.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsp = data_set.pipeline('dsp')\n",
    "dsp.reset() # see below for an explanation of pipeline reset\n",
    "dsp.add_step(add_one)\n",
    "data_set.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the Python kernel is reset, all in-memory data is cleared. However, the dataset, along with its pipeline, is persisted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING!! this cell resets the kernel, wiping out everything!\n",
    "%reset -f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the test pipeline is gone:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    tp\n",
    "except NameError:\n",
    "    print('No tp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we demonstrate that the `sample` dataset is persisted as is the `dsp` pipeline. In order to show how the pipeline persists, re-establish the required libraries and the Cortex builder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from cortex import Cortex\n",
    "\n",
    "cortex = Cortex.local()\n",
    "builder = cortex.builder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the dataset, using the name we previousily specified:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = cortex.dataset('sample')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the pipeline, again using the name we previousily specified:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pipeline = data_set.pipeline('dsp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the dataframe from the dataset and then run the pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_set.as_pandas()\n",
    "new_pipeline.run(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the `add_one` function is still present in the `new_pipeline`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reset() affects pipeline state\n",
    "\n",
    "Without parameters, reset removes all steps in the pipline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pipeline.to_camel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pipeline.reset()\n",
    "\n",
    "new_pipeline.to_camel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also delete all context and dependencies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "another_new_pl = data_set.pipeline('dsp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "another_new_pl.set_context('for_example','a string')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a dependency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = builder.pipeline('test_pipeline')\n",
    "another_new_pl.add_dependency(tp)\n",
    "\n",
    "another_new_pl.to_camel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next reset everything, including pipeline steps, dependencies, and context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "another_new_pl.reset(reset_deps=True, reset_context=True)\n",
    "\n",
    "another_new_pl.to_camel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
