{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ames Housing Prices - Step 2: Data Prep\n",
    "In this phase, we will build a data prep pipeline to handle outliers and missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic setup\n",
    "%run ./config.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Cortex 5\n",
    "cortex = Cortex.client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve training dataset from Cortex\n",
    "train_ds = cortex.dataset('kaggle/ames-housing-train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines\n",
    "Each Cortex 5 Dataset can have multiple named pipelines which are chains of Python functions that transfor the dataset for specific purposes (e.g. data cleaning, feature prep, feature framing, etc.).  We begin here by creating a _clean_ pipeline that will handle remove unwanted columns, handle missing data, remove outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = train_ds.pipeline('clean', clear_cache=True)\n",
    "pipeline.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline Steps\n",
    "Each step in the pipeline is a Python function that accepts and instance of the pipeline object and a Pandas DataFrame for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_unused(pipeline, df):\n",
    "    df.drop(columns=['Id'], axis=1, inplace=True)\n",
    "\n",
    "pipeline.add_step(drop_unused)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_outliers(pipeline, df):\n",
    "    df.drop(df[(df['GrLivArea'].astype(int)>4000) & (df['SalePrice'].astype(int)<300000)].index, inplace=True)\n",
    "    \n",
    "pipeline.add_step(drop_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_zero_cols(pipeline, df):\n",
    "    fill_zero_cols = ['BsmtHalfBath', 'BsmtFullBath', 'BsmtFinSF2', 'GarageCars']\n",
    "    [df[i].fillna(0, inplace=True) for i in fill_zero_cols]\n",
    "\n",
    "pipeline.add_step(fill_zero_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Pipeline Memory\n",
    "Each pipeline has a memory, called _context_ that is useful for recording state that needs to be used later.  In this case, we are recording the median values of several columns that will be used to fill in missing values for those columns.  We need these median values later when encounter test instances and need to fill in the same missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_median_cols(pipeline, df):\n",
    "    fill_median_cols = ['GarageArea','TotalBsmtSF', 'MasVnrArea', 'BsmtFinSF1', 'LotFrontage', 'BsmtUnfSF', 'GarageYrBlt']\n",
    "    [pipeline.set_context('{}_median'.format(j), np.asscalar(df[j].astype(float).median())) for j in fill_median_cols]\n",
    "    [df[j].fillna(df[j].astype(float).median(), inplace=True) for j in fill_median_cols]\n",
    "    \n",
    "pipeline.add_step(fill_median_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_na_none(pipeline, df):\n",
    "    df.fillna('none', inplace=True)\n",
    "\n",
    "pipeline.add_step(fill_na_none)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Pipeline\n",
    "The pipeline _run_ method will run each of our steps in order and return a transformed DataFrame instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Lets validate that our pipeline solved our missing data and outliers problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz = train_ds.visuals(train_df, figsize=(24, 9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.show_missing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.show_corr_pairs('SalePrice', threshold=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
