{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why annotate experiments?\n",
    "Because most choices a data scientist makes for one model cannot be carried over to a new model or experiment, data may need exploration and manipulation to produce a set of features a model can use. Models often have many parameters so that they are not amenable to formal analysis. \n",
    "\n",
    "The Cortex Python SDK provides facilites for these activities. The priniciple abstraction to support these tasks is the experiment. The user can create a collection of `runs` that systematically explore different views of the data and different parameters for the predective models. Runs can be annotated with parameters, metrics, artifacts and metadata. \n",
    "\n",
    "Parameters describe how the model is configured. Metrics describe how well the model did in making predictions. Artifacts are a place to keep other information or software objects associated with a particular run. Metadata is for any information about the information in the run. \n",
    "\n",
    "This notebook demonstrates how to use annotations for `runs` and `experiments`.\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow==1.12.0\n",
    "!pip install keras==2.2.2\n",
    "\n",
    "import numpy\n",
    "import pandas\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from cortex import Cortex\n",
    "\n",
    "client = Cortex.local()\n",
    "exp = client.experiment('example/experiment-anno')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example uses the same data set that was used in the basic experiment notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pandas.read_csv('./data/iris.data')\n",
    "dataset = df.values\n",
    "ds_classes = pandas.get_dummies(df).values # one-hot encode the classes\n",
    "\n",
    "X = dataset[:,0:4].astype(float)\n",
    "Y = ds_classes[:,4:8].astype(int)\n",
    "\n",
    "(train_inputs, test_inputs, train_classes, test_classes) = train_test_split(X, Y, test_size=0.333, train_size=0.667)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment models\n",
    "\n",
    "This example uses a neural network model. Neural networks have many possible configurations and model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lf = 'categorical_crossentropy'\n",
    "\n",
    "def adam_opt_model():\n",
    "    model = Sequential()\n",
    "    model.name = 'adam optimizer'\n",
    "    model.add(Dense(16, input_dim=4, activation='relu'))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    model.compile(loss=lf, optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a second model with a different layer structure and a different optimizer function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def small_sgd_opt_model():\n",
    "    model = Sequential()\n",
    "    model.name = 'small sgd optimizer'\n",
    "    model.add(Dense(8, input_dim=4, activation='relu'))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    model.compile(loss=lf, optimizer='sgd', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotate the experiment\n",
    "\n",
    "To capture some of the hyperparameters related to this run, the experiment is annotated: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.meta = {'loss function':lf, 'nn type':'sequential'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotate runs \n",
    "\n",
    "The `runs` in this experiment explore the results given by the two different models. Training results will vary as you vary the batch size and number of training epochs. For brevitiy, this notebook only tests one batch size and one epoch count, but you can uncomment the lines relating to epoch_cts to see the difference in batch runtime. This experiment is using [cross validation](https://scikit-learn.org/stable/modules/cross_validation.html) to compare models trained with different subsets of the data to improve the accuracy of the metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_cts = [1, 10]\n",
    "batch_sz = 3\n",
    "\n",
    "for model in [adam_opt_model, small_sgd_opt_model]:\n",
    "    for epoch_ct in epoch_cts:\n",
    "        with exp.start_run() as run: \n",
    "            estimator = KerasClassifier(build_fn=model, epochs=epoch_ct, batch_size=batch_sz, verbose=0)\n",
    "            kfold = KFold(n_splits=3, shuffle=True)\n",
    "            results = cross_val_score(estimator, test_inputs, test_classes, cv=kfold)\n",
    "            run.log_artifact('model', model)\n",
    "            run.log_param('batch size', batch_sz)\n",
    "            run.log_param('epochs', epoch_ct)\n",
    "            run.log_param('model name', model().name)\n",
    "            run.log_metric('mean % acc', results.mean()*100)\n",
    "            run.log_metric('margin of err', results.std()*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining the experiment shows how the two models compare. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use artifacts\n",
    "\n",
    "Examine the table for speed, accuracy, and the lowest prediction errors. Models that performed well can be retrieved and used for predictions or for further experiments. In the following example, get the last run to make a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = exp.last_run()\n",
    "model = run.get_artifact('model')\n",
    "\n",
    "sample = numpy.array([[4.9,3.1,1.5,0.2]]) # sample of one\n",
    "\n",
    "pred = model().predict(sample)\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Undo the one hot encoding variable, and display the model's prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = numpy.argmax(pred, axis=1).item(0)\n",
    "\n",
    "iris_dict = {0:'Iris-setosa',1:'Iris-versicolor',2:'Iris-virginica'}\n",
    "\n",
    "iris_dict[x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
